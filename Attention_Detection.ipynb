{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6e8f81d9c4f41acb7480b46befe39a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_523b47490b1b4196a4f9fae3e0a2a661",
              "IPY_MODEL_6f184aa18cc34d2fb626ccaa4c7ff38d",
              "IPY_MODEL_2fb1cd90ca5e4e70a124761b0a9195cc"
            ],
            "layout": "IPY_MODEL_7338958a8265424791157f036e88715d"
          }
        },
        "523b47490b1b4196a4f9fae3e0a2a661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b251f9afa7b4bbc9dc0542d5f15f1e9",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf9d5fad2eb41a38837869e1208c32a",
            "value": "model.safetensors: 100%"
          }
        },
        "6f184aa18cc34d2fb626ccaa4c7ff38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2643a4c0509463ea2315b77102a98e9",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee4953033c0474482ffd2be63ee292e",
            "value": 346284714
          }
        },
        "2fb1cd90ca5e4e70a124761b0a9195cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8d729f8ec141a29c73a6d95afa81c6",
            "placeholder": "​",
            "style": "IPY_MODEL_29dafd541b7d4fda9ddd50bb58cb12db",
            "value": " 346M/346M [00:01&lt;00:00, 235MB/s]"
          }
        },
        "7338958a8265424791157f036e88715d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b251f9afa7b4bbc9dc0542d5f15f1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf9d5fad2eb41a38837869e1208c32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2643a4c0509463ea2315b77102a98e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee4953033c0474482ffd2be63ee292e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d8d729f8ec141a29c73a6d95afa81c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29dafd541b7d4fda9ddd50bb58cb12db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iOMgQLzqqg36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fff029-0f16-4bd5-dac2-6207376c6592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping C1_S001_P001_AD4: No arousal value below -250.\n",
            "Skipping C1_S001_P002_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S002_P003_AD4/Round1_SessionId_111_VideoId_15_UserId_7a52e85b_cdd1_4a0d_8918_5cc4703c5b69_Arousal_Arousal1.csv\n",
            "Skipping C1_S002_P004_AD4: No arousal value below -250.\n",
            "Skipping C1_S003_P005_AD4: No arousal value below -250.\n",
            "Skipping C1_S003_P006_AD4: No arousal value below -250.\n",
            "Skipping C1_S004_P007_AD4: No arousal value below -250.\n",
            "Skipping C1_S004_P008_AD4: No arousal value below -250.\n",
            "Skipping C1_S005_P009_AD4: No arousal value below -250.\n",
            "Skipping C1_S005_P010_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S006_P011_AD4/Round1_SessionId_130_VideoId_15_UserId_0e51ac1f_2fd5_456d_b8a2_d2b4dcfc0fd9_Arousal_Arousal1.csv\n",
            "Skipping C1_S006_P012_AD4: No arousal value below -250.\n",
            "Skipping C1_S007_P013_AD4: No arousal value below -250.\n",
            "Skipping C1_S007_P014_AD4: No arousal value below -250.\n",
            "Skipping C1_S008_P015_AD4: No arousal value below -250.\n",
            "Skipping C1_S008_P016_AD4: No arousal value below -250.\n",
            "Skipping C1_S009_P017_AD4: No arousal value below -250.\n",
            "Skipping C1_S009_P018_AD4: No arousal value below -250.\n",
            "Skipping C1_S010_P019_AD4: No arousal value below -250.\n",
            "Skipping C1_S010_P020_AD4: No arousal value below -250.\n",
            "Skipping C1_S011_P021_AD4: No arousal value below -250.\n",
            "Skipping C1_S011_P022_AD4: No arousal value below -250.\n",
            "Skipping C1_S012_P023_AD4: No arousal value below -250.\n",
            "Skipping C1_S012_P024_AD4: No arousal value below -250.\n",
            "Skipping C1_S013_P025_AD4: No arousal value below -250.\n",
            "Skipping C1_S013_P026_AD4: No arousal value below -250.\n",
            "Skipping C1_S014_P027_AD4: No arousal value below -250.\n",
            "Skipping C1_S014_P028_AD4: No arousal value below -250.\n",
            "Skipping C1_S015_P029_AD4: No arousal value below -250.\n",
            "Skipping C1_S015_P030_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S016_P031_AD4/Round1_SessionId_309_VideoId_15_UserId_ca191f57_ee6d_411e_b5d8_551647f45550_Arousal_Arousal1.csv\n",
            "Skipping C1_S016_P032_AD4: No arousal value below -250.\n",
            "Skipping C1_S017_P033_AD4: No arousal value below -250.\n",
            "Skipping C1_S017_P034_AD4: No arousal value below -250.\n",
            "Skipping C1_S018_P035_AD4: No arousal value below -250.\n",
            "Skipping C1_S018_P036_AD4: No arousal value below -250.\n",
            "Skipping C1_S019_P037_AD4: No arousal value below -250.\n",
            "Skipping C1_S019_P038_AD4: No arousal value below -250.\n",
            "Skipping C1_S020_P039_AD4: No arousal value below -250.\n",
            "Skipping C1_S020_P040_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S021_P041_AD4/Round1_SessionId_279_VideoId_15_UserId_e191aeaf_b2a7_496b_845a_82f6f3145f6e_Arousal_Arousal1.csv\n",
            "Skipping C1_S021_P042_AD4: No arousal value below -250.\n",
            "Skipping C1_S022_P043_AD4: No arousal value below -250.\n",
            "Skipping C1_S022_P044_AD4: No arousal value below -250.\n",
            "Skipping C1_S023_P045_AD4: No arousal value below -250.\n",
            "Skipping C1_S023_P046_AD4: No arousal value below -250.\n",
            "Skipping C1_S024_P047_AD4: No arousal value below -250.\n",
            "Skipping C1_S024_P048_AD4: No arousal value below -250.\n",
            "Skipping C1_S025_P049_AD4: No arousal value below -250.\n",
            "Skipping C1_S025_P050_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S026_P051_AD4/Round1_SessionId_363_VideoId_15_UserId_f320c544_887b_4e6a_a627_f5c085dda7da_Arousal_Arousal1.csv\n",
            "Skipping C1_S026_P052_AD4: No arousal value below -250.\n",
            "Skipping C1_S027_P053_AD4: No arousal value below -250.\n",
            "Skipping C1_S027_P054_AD4: No arousal value below -250.\n",
            "Skipping C1_S028_P055_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C1_S028_P056_AD4/Round1_SessionId_212_VideoId_15_UserId_b0277448_a11f_4317_884e_61098482d0bf_Arousal_Arousal1.csv\n",
            "Skipping C1_S029_P057_AD4: No arousal value below -250.\n",
            "Skipping C1_S029_P058_AD4: No arousal value below -250.\n",
            "Skipping C1_S030_P059_AD4: No arousal value below -250.\n",
            "Skipping C1_S030_P060_AD4: No arousal value below -250.\n",
            "Skipping C1_S031_P061_AD4: No arousal value below -250.\n",
            "Skipping C1_S031_P062_AD4: No arousal value below -250.\n",
            "Skipping C1_S032_P063_AD4: No arousal value below -250.\n",
            "Skipping C1_S032_P064_AD4: No arousal value below -250.\n",
            "Skipping C1_S192_P383_AD4: No arousal value below -250.\n",
            "Skipping C1_S192_P384_AD4: No arousal value below -250.\n",
            "Skipping C1_S193_P385_AD4: No arousal value below -250.\n",
            "Skipping C1_S193_P386_AD4: No arousal value below -250.\n",
            "Skipping C1_S194_P387_AD4: No arousal value below -250.\n",
            "Skipping C1_S194_P388_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S033_P065_AD4/Round1_SessionId_271_VideoId_4_UserId_13c59d07_adab_4811_9f13_3dbfcd5eebfb_Arousal_Arousal1.csv\n",
            "Skipping C2_S033_P066_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S034_P067_AD4/Round1_SessionId_190_VideoId_4_UserId_5b9cb2e1_56b2_4578_88fe_b7d625b413c0_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S034_P068_AD4/Round1_SessionId_190_VideoId_4_UserId_d3c02b09_082d_45f2_983f_67e2e529b1cb_Arousal_Arousal1.csv\n",
            "Skipping C2_S035_P069_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S035_P070_AD4/Round1_SessionId_183_VideoId_4_UserId_da08b3d7_ad82_4883_9185_a11b151bbdcb_Arousal_Arousal1.csv\n",
            "Skipping C2_S036_P071_AD4: No arousal value below -250.\n",
            "Skipping C2_S036_P072_AD4: No arousal value below -250.\n",
            "Skipping C2_S037_P073_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S037_P074_AD4/Round1_SessionId_135_VideoId_4_UserId_ed06e59c_9ec0_49a9_8edf_2c0b45efe76b_Arousal_Arousal1.csv\n",
            "Skipping C2_S038_P075_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S038_P076_AD4/Round1_SessionId_364_VideoId_4_UserId_fffd88a9_d6b2_485b_b4c9_14b0b6001f50_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S039_P077_AD4/Round1_SessionId_132_VideoId_4_UserId_9ff65e93_10f2_4d6f_8b14_257f14e94a54_Arousal_Arousal1.csv\n",
            "Skipping C2_S039_P078_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S040_P079_AD4/Round1_SessionId_310_VideoId_4_UserId_487ffc14_f1db_475b_a120_1ae31225741f_Arousal_Arousal1.csv\n",
            "Skipping C2_S040_P080_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S041_P081_AD4/Round1_SessionId_131_VideoId_4_UserId_14c0aa28_37af_4e8e_b982_69ff741224d9_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S041_P082_AD4/Round1_SessionId_131_VideoId_4_UserId_7d944d53_e079_4611_a3e1_69ada22c71ba_Arousal_Arousal1.csv\n",
            "Skipping C2_S042_P083_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S042_P084_AD4/Round1_SessionId_166_VideoId_4_UserId_2911284a_8007_4385_8db0_a1f6a3d9a8e3_Arousal_Arousal1.csv\n",
            "Skipping C2_S043_P085_AD4: No arousal value below -250.\n",
            "Skipping C2_S043_P086_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S044_P087_AD4/Round1_SessionId_302_VideoId_4_UserId_0cb9828e_8810_4d6f_aea0_18705c5084a0_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S044_P088_AD4/Round1_SessionId_302_VideoId_4_UserId_5e4b2c24_2246_4c49_8510_47100d729d31_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S045_P089_AD4/Round1_SessionId_157_VideoId_4_UserId_c1f7d6f6_4205_487b_82c5_f68b6b102d85_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S045_P090_AD4/Round1_SessionId_157_VideoId_4_UserId_3219b844_ec76_47a0_9030_96ff601edb52_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S046_P091_AD4/Round1_SessionId_344_VideoId_4_UserId_142a9d6d_b7f9_47fc_8beb_a78112d4115b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S046_P092_AD4/Round1_SessionId_344_VideoId_4_UserId_b9542177_bd68_445c_b9c0_68ec469f9d7b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S047_P093_AD4/Round1_SessionId_400_VideoId_4_UserId_6e240eec_e6c0_4ca6_990d_05063c6731a2_Arousal_Arousal1.csv\n",
            "Skipping C2_S047_P094_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S048_P095_AD4/Round1_SessionId_408_VideoId_4_UserId_96304cb5_f965_4478_bfea_8a7e7a05e62b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S048_P096_AD4/Round1_SessionId_408_VideoId_4_UserId_69157436_8c42_43f8_b8da_9f3fa32d3932_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S049_P097_AD4/Round1_SessionId_482_VideoId_4_UserId_b3fe0e79_5287_4575_b42a_a881a2c16f8a_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S049_P098_AD4/Round1_SessionId_482_VideoId_4_UserId_549f373c_3954_42dd_9949_bb3f7cbdcb41_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S050_P099_AD4/Round1_SessionId_507_VideoId_4_UserId_095c6ae0_49c9_42af_abd6_348efb14a3f9_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S050_P100_AD4/Round1_SessionId_507_VideoId_4_UserId_fd1a319b_fc0a_497d_b218_c5456f1f0d0f_Arousal_Arousal1.csv\n",
            "Skipping C2_S051_P101_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S051_P102_AD4/Round1_SessionId_508_VideoId_4_UserId_c93a18b2_e4d3_4880_add8_5249b7c94bff_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S052_P103_AD4/Round1_SessionId_513_VideoId_4_UserId_f5d898f3_7ac7_439e_8e0b_2bf2a3134f8b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S052_P104_AD4/Round1_SessionId_513_VideoId_4_UserId_3d20bdd8_d542_4aa5_aeed_c2295c3dcc4d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S053_P105_AD4/Round1_SessionId_527_VideoId_4_UserId_fcfef37b_4fd2_4ab5_a18d_5d97cb9065ad_Arousal_Arousal1.csv\n",
            "Skipping C2_S053_P106_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S054_P107_AD4/Round1_SessionId_1527_VideoId_4_UserId_06b5a855_b073_40ab_92c8_7051d2405a1d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S054_P108_AD4/Round1_SessionId_1527_VideoId_4_UserId_4510e3ce_012f_4104_b6dc_7f9f738c147b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S055_P109_AD4/Round1_SessionId_1528_VideoId_4_UserId_84d9b883_873f_4404_9b75_5ab318e24d5c_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S055_P110_AD4/Round1_SessionId_1528_VideoId_4_UserId_46182600_a8a8_40e4_8716_5811973f4e7d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S056_P111_AD4/Round1_SessionId_1529_VideoId_4_UserId_a4169bc2_36be_4773_a5a1_06a3a5b02b1a_Arousal_Arousal1.csv\n",
            "Skipping C2_S056_P112_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S057_P113_AD4/Round1_SessionId_1530_VideoId_4_UserId_c9aed654_b660_4e95_ad6e_4512404edb23_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S057_P114_AD4/Round1_SessionId_1530_VideoId_4_UserId_39d8313e_9715_4b4c_8e73_3a81cd071406_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S058_P115_AD4/Round1_SessionId_121_VideoId_4_UserId_1898a811_66f6_420c_a3cf_b830e07c34ff_Arousal_Arousal1.csv\n",
            "Skipping C2_S058_P116_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S059_P117_AD4/Round1_SessionId_1531_VideoId_4_UserId_a6a19e1a_3342_4aee_b0b1_809a9b6d64cd_Arousal_Arousal1.csv\n",
            "Skipping C2_S059_P118_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S060_P119_AD4/Round1_SessionId_146_VideoId_4_UserId_9075c9db_d739_4ee6_99bc_bd75b6622c29_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S060_P120_AD4/Round1_SessionId_146_VideoId_4_UserId_d63f6114_b5b0_4461_80c9_80bf7d69dffb_Arousal_Arousal1.csv\n",
            "Skipping C2_S061_P121_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S061_P122_AD4/Round1_SessionId_147_VideoId_4_UserId_eca490a9_afae_481c_b5ff_92d1e6712d44_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S062_P123_AD4/Round1_SessionId_149_VideoId_4_UserId_ca8166ba_39d3_46a1_97c2_6c589cbb25e4_Arousal_Arousal1.csv\n",
            "Skipping C2_S062_P124_AD4: No arousal value below -250.\n",
            "Skipping C2_S195_P389_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S195_P390_AD4/Round1_SessionId_1541_VideoId_4_UserId_1559fdd1_fe05_4a16_9fca_1547a6672a0c_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S196_P391_AD4/Round1_SessionId_1542_VideoId_4_UserId_fbbf6b4a_8205_4618_8e26_f75d0db95f27_Arousal_Arousal1.csv\n",
            "Skipping C2_S196_P392_AD4: No arousal value below -250.\n",
            "Skipping C2_S197_P393_AD4: No arousal value below -250.\n",
            "Skipping C2_S197_P394_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S198_P395_AD4/Round1_SessionId_1553_VideoId_4_UserId_4117b5c1_c77c_4d87_ac0c_25d928e27fe5_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S198_P396_AD4/Round1_SessionId_1553_VideoId_4_UserId_0f67c9a7_aaba_4878_be1f_ee90c586a2c8_Arousal_Arousal1.csv\n",
            "Skipping C2_S199_P397_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C2_S199_P398_AD4/Round1_SessionId_1560_VideoId_4_UserId_a76d0a7e_ada4_40ca_b531_bff6d426566f_Arousal_Arousal1.csv\n",
            "Skipping C3_S063_P125_AD4: No arousal value below -250.\n",
            "Skipping C3_S063_P126_AD4: No arousal value below -250.\n",
            "Skipping C3_S064_P127_AD4: No arousal value below -250.\n",
            "Skipping C3_S064_P128_AD4: No arousal value below -250.\n",
            "Skipping C3_S065_P129_AD4: No arousal value below -250.\n",
            "Skipping C3_S065_P130_AD4: No arousal value below -250.\n",
            "Skipping C3_S066_P131_AD4: No arousal value below -250.\n",
            "Skipping C3_S066_P132_AD4: No arousal value below -250.\n",
            "Skipping C3_S067_P133_AD4: No arousal value below -250.\n",
            "Skipping C3_S067_P134_AD4: No arousal value below -250.\n",
            "Skipping C3_S068_P135_AD4: No arousal value below -250.\n",
            "Skipping C3_S068_P136_AD4: No arousal value below -250.\n",
            "Skipping C3_S069_P137_AD4: No arousal value below -250.\n",
            "Skipping C3_S069_P138_AD4: No arousal value below -250.\n",
            "Skipping C3_S070_P139_AD4: No arousal value below -250.\n",
            "Skipping C3_S070_P140_AD4: No arousal value below -250.\n",
            "Skipping C3_S071_P141_AD4: No arousal value below -250.\n",
            "Skipping C3_S071_P142_AD4: No arousal value below -250.\n",
            "Skipping C3_S072_P143_AD4: No arousal value below -250.\n",
            "Skipping C3_S072_P144_AD4: No arousal value below -250.\n",
            "Skipping C3_S073_P145_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C3_S073_P146_AD4/Round1_SessionId_332_VideoId_16_UserId_a783b375_2283_42e9_9ba3_2135d89cc1b1_Arousal_Arousal1.csv\n",
            "Skipping C3_S074_P147_AD4: No arousal value below -250.\n",
            "Skipping C3_S074_P148_AD4: No arousal value below -250.\n",
            "Skipping C3_S075_P149_AD4: No arousal value below -250.\n",
            "Skipping C3_S075_P150_AD4: No arousal value below -250.\n",
            "Skipping C3_S076_P151_AD4: No arousal value below -250.\n",
            "Skipping C3_S076_P152_AD4: No arousal value below -250.\n",
            "Skipping C3_S077_P153_AD4: No arousal value below -250.\n",
            "Skipping C3_S077_P154_AD4: No arousal value below -250.\n",
            "Skipping C3_S078_P155_AD4: No arousal value below -250.\n",
            "Skipping C3_S078_P156_AD4: No arousal value below -250.\n",
            "Skipping C3_S079_P157_AD4: No arousal value below -250.\n",
            "Skipping C3_S079_P158_AD4: No arousal value below -250.\n",
            "Skipping C3_S080_P159_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C3_S080_P160_AD4/Round1_SessionId_373_VideoId_16_UserId_f6dfff0d_3973_4e9a_bb02_19607139e04f_Arousal_Arousal1.csv\n",
            "Skipping C3_S081_P161_AD4: No arousal value below -250.\n",
            "Skipping C3_S081_P162_AD4: No arousal value below -250.\n",
            "Skipping C3_S082_P163_AD4: No arousal value below -250.\n",
            "Skipping C3_S082_P164_AD4: No arousal value below -250.\n",
            "Skipping C3_S083_P165_AD4: No arousal value below -250.\n",
            "Skipping C3_S083_P166_AD4: No arousal value below -250.\n",
            "Skipping C3_S084_P167_AD4: No arousal value below -250.\n",
            "Skipping C3_S084_P168_AD4: No arousal value below -250.\n",
            "Skipping C3_S085_P169_AD4: No arousal value below -250.\n",
            "Skipping C3_S085_P170_AD4: No arousal value below -250.\n",
            "Skipping C3_S086_P171_AD4: No arousal value below -250.\n",
            "Skipping C3_S086_P172_AD4: No arousal value below -250.\n",
            "Skipping C3_S087_P173_AD4: No arousal value below -250.\n",
            "Skipping C3_S087_P174_AD4: No arousal value below -250.\n",
            "Skipping C3_S088_P175_AD4: No arousal value below -250.\n",
            "Skipping C3_S088_P176_AD4: No arousal value below -250.\n",
            "Skipping C3_S089_P177_AD4: No arousal value below -250.\n",
            "Skipping C3_S089_P178_AD4: No arousal value below -250.\n",
            "Skipping C3_S090_P179_AD4: No arousal value below -250.\n",
            "Skipping C3_S090_P180_AD4: No arousal value below -250.\n",
            "Skipping C3_S091_P181_AD4: No arousal value below -250.\n",
            "Skipping C3_S091_P182_AD4: No arousal value below -250.\n",
            "Skipping C3_S092_P183_AD4: No arousal value below -250.\n",
            "Skipping C3_S092_P184_AD4: No arousal value below -250.\n",
            "Skipping C3_S093_P185_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C3_S093_P186_AD4/Round1_SessionId_375_VideoId_16_UserId_6d937bd6_9775_48fa_9da1_576d148708ff_Arousal_Arousal1.csv\n",
            "Skipping C3_S094_P187_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C3_S094_P188_AD4/Round1_SessionId_329_VideoId_16_UserId_d9c5ae05_8799_42bd_abbe_cd05b47e4bb8_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S095_P189_AD4/Round1_SessionId_383_VideoId_4_UserId_63458cbe_4561_4944_8f56_75545c214e2c_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S095_P190_AD4/Round1_SessionId_383_VideoId_4_UserId_a67cf20f_43e1_4279_ab33_2a31470e6086_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S096_P191_AD4/Round1_SessionId_39_VideoId_4_UserId_6bb55080_9de9_4d1c_a834_45a417e17058_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S096_P192_AD4/Round1_SessionId_39_VideoId_4_UserId_70f7d0f5_39c0_40c1_b66c_dd7cd462240a_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S097_P193_AD4/Round1_SessionId_56_VideoId_4_UserId_3220be66_a58d_44a7_b6c1_409e217cb039_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S097_P194_AD4/Round1_SessionId_56_VideoId_4_UserId_315a48b2_8706_4d45_9403_49660fabd22d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S098_P195_AD4/Round1_SessionId_159_VideoId_4_UserId_956fd960_b905_4655_85f3_7c5cc33fd233_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S098_P196_AD4/Round1_SessionId_159_VideoId_4_UserId_eb2e90f5_905f_496d_b8f8_1841913ed6ed_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S099_P197_AD4/Round1_SessionId_41_VideoId_4_UserId_79cb0525_63d0_46d0_b746_d07bbaa8da67_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S099_P198_AD4/Round1_SessionId_41_VideoId_4_UserId_18a2541f_8a9d_4e53_beb4_9a66325154db_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S100_P199_AD4/Round1_SessionId_35_VideoId_4_UserId_363911dd_e338_45cf_9537_0e27bbf5bc21_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S100_P200_AD4/Round1_SessionId_35_VideoId_4_UserId_128fd62b_0dd0_4b40_982b_72d971f3a953_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S101_P201_AD4/Round1_SessionId_176_VideoId_4_UserId_5b662953_7633_4269_a49a_5b5d0465f9d3_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S101_P202_AD4/Round1_SessionId_176_VideoId_4_UserId_673fcb89_e4f8_4860_8aa7_b5bce8b2bee7_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S102_P203_AD4/Round1_SessionId_308_VideoId_4_UserId_e7962fa3_3f09_4239_9f20_5ecd0a4a45ce_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S102_P204_AD4/Round1_SessionId_308_VideoId_4_UserId_3cb46837_8fcd_4ebb_be4f_97de355c44d6_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S103_P205_AD4/Round1_SessionId_436_VideoId_4_UserId_4bff259e_974b_4ef1_b710_e2da01cd196f_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S103_P206_AD4/Round1_SessionId_436_VideoId_4_UserId_da753f10_689c_40d4_9714_1d14b4eff942_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S104_P207_AD4/Round1_SessionId_110_VideoId_4_UserId_882afe05_e991_4289_8a3c_33d965dc9b93_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S104_P208_AD4/Round1_SessionId_110_VideoId_4_UserId_c9b376b1_1b11_489d_a6cb_6ed433cde42b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S105_P209_AD4/Round1_SessionId_188_VideoId_4_UserId_d3cc9685_4312_4fe6_bddc_a3c21f35a614_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S105_P210_AD4/Round1_SessionId_188_VideoId_4_UserId_9cb1db24_50ff_4353_8cb0_c69f1ed6ac12_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S106_P211_AD4/Round1_SessionId_191_VideoId_4_UserId_c80892ec_12fc_4276_9c46_9c5b908466e3_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S106_P212_AD4/Round1_SessionId_191_VideoId_4_UserId_114535d4_918a_42b9_a934_b84285229faf_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S107_P213_AD4/Round1_SessionId_192_VideoId_4_UserId_6d55e757_cfef_4db6_8073_2962c747d923_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S107_P214_AD4/Round1_SessionId_192_VideoId_4_UserId_f6c9a2a2_436e_4800_b3ec_29476764b524_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S108_P215_AD4/Round1_SessionId_206_VideoId_4_UserId_0d2867ac_de24_49f9_85ce_15cb4c5c1d9c_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S108_P216_AD4/Round1_SessionId_206_VideoId_4_UserId_43df5c36_152f_4c85_9608_bbf73d861f28_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S109_P217_AD4/Round1_SessionId_407_VideoId_4_UserId_9ea43fe4_725c_4fad_8f6b_089f48a8c3e7_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S109_P218_AD4/Round1_SessionId_407_VideoId_4_UserId_b9c949e4_d033_49d9_8bfc_d96208fd51ba_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S110_P219_AD4/Round1_SessionId_198_VideoId_4_UserId_385ac018_14d7_459e_bf02_00074ba2138e_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S110_P220_AD4/Round1_SessionId_198_VideoId_4_UserId_9d1b7bec_d802_496c_bfab_b29985581763_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S111_P221_AD4/Round1_SessionId_203_VideoId_4_UserId_bbcd2d3c_3bd1_416a_90dc_bb9d67247755_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S111_P222_AD4/Round1_SessionId_203_VideoId_4_UserId_9bb4f71e_c43e_4bf4_82ab_343fb85cd885_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S112_P223_AD4/Round1_SessionId_202_VideoId_4_UserId_d41ada44_fa2e_47b5_a7ad_11931df41be2_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S112_P224_AD4/Round1_SessionId_202_VideoId_4_UserId_b74c6923_031d_4307_a70d_8b928f83105a_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S113_P225_AD4/Round1_SessionId_209_VideoId_4_UserId_944a48ff_89c4_4745_a4f6_36ca4ff570df_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S113_P226_AD4/Round1_SessionId_209_VideoId_4_UserId_30ee62f9_2666_45c9_82f0_de78d3d178c5_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S114_P227_AD4/Round1_SessionId_417_VideoId_4_UserId_eb35ccaa_ab5f_4e3f_a0ee_ea7cb8391e26_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S114_P228_AD4/Round1_SessionId_417_VideoId_4_UserId_74406614_ec5f_47c1_adc0_b6a631877db4_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S115_P229_AD4/Round1_SessionId_351_VideoId_4_UserId_70944354_0e9c_44e4_ae78_b8db7b213796_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S115_P230_AD4/Round1_SessionId_351_VideoId_4_UserId_565f9bf3_dc6a_48c4_9fbe_e72eab86f995_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S116_P231_AD4/Round1_SessionId_435_VideoId_4_UserId_155a88fa_d102_433b_a45d_f995f7ac4c5b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S116_P232_AD4/Round1_SessionId_435_VideoId_4_UserId_f347f5ce_8b74_40ed_9ce2_d7e43bf02473_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S117_P233_AD4/Round1_SessionId_485_VideoId_4_UserId_c5a71447_ada6_4a8b_b404_ae91fcaf6c50_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S117_P234_AD4/Round1_SessionId_485_VideoId_4_UserId_09e7d050_86e9_4041_a14f_b6584e38e0fb_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S118_P235_AD4/Round1_SessionId_439_VideoId_4_UserId_69a79700_bee5_41a9_8fff_35b8a66372e9_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S118_P236_AD4/Round1_SessionId_439_VideoId_4_UserId_a5f3b3f9_b25f_4d0a_9b0f_aca768d67c6b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S119_P237_AD4/Round1_SessionId_441_VideoId_4_UserId_25810d52_a383_43bf_aca6_609ce1885383_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S119_P238_AD4/Round1_SessionId_441_VideoId_4_UserId_8f534e2f_f8bb_48c4_9019_bd33062231bb_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S120_P239_AD4/Round1_SessionId_442_VideoId_4_UserId_899b1acd_7488_496f_9472_7c6ea72c8803_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S120_P240_AD4/Round1_SessionId_442_VideoId_4_UserId_ddca1a24_4920_4b1a_9841_34d402c25de4_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S121_P241_AD4/Round1_SessionId_479_VideoId_4_UserId_ec1ede38_ac33_488f_9084_4ca78fa75c3a_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S121_P242_AD4/Round1_SessionId_479_VideoId_4_UserId_9899f6d9_7663_4f90_9308_b851e56ed49d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S122_P243_AD4/Round1_SessionId_480_VideoId_4_UserId_cf0b5f28_b748_4b1d_aa26_e282ffbf0341_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S122_P244_AD4/Round1_SessionId_480_VideoId_4_UserId_b2ce0850_09ee_4acf_b175_3c7daeca8113_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S123_P245_AD4/Round1_SessionId_478_VideoId_4_UserId_91ec678c_e007_49ba_ab7a_cfee1cfb842f_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S123_P246_AD4/Round1_SessionId_478_VideoId_4_UserId_8a406c44_e40c_4b2b_8fe2_523a4f292906_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S124_P247_AD4/Round1_SessionId_460_VideoId_4_UserId_3d962b82_35af_4e67_abec_da9a615b0c9b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S124_P248_AD4/Round1_SessionId_460_VideoId_4_UserId_a25ae4f2_3ef0_4698_b2c4_f0c97327e896_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S125_P249_AD4/Round1_SessionId_491_VideoId_4_UserId_2218557a_ed77_43ef_a093_c8d37b35e8c6_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S125_P250_AD4/Round1_SessionId_491_VideoId_4_UserId_3cfa75ea_629d_41aa_8f68_f1dc6eea4df9_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S126_P251_AD4/Round1_SessionId_505_VideoId_4_UserId_587a52f6_30f8_43a5_86f2_e580e8f11a89_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S126_P252_AD4/Round1_SessionId_505_VideoId_4_UserId_fb9b2dfc_6c9d_4461_b06a_ce395c3ba75b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C4_S127_P254_AD4/Round1_SessionId_526_VideoId_4_UserId_52258f68_a6f2_42ea_b1e3_0083d2f9b6e5_Arousal_Arousal1.csv\n",
            "Skipping C5_S128_P255_AD4: No arousal value below -250.\n",
            "Skipping C5_S128_P256_AD4: No arousal value below -250.\n",
            "Skipping C5_S129_P257_AD4: No arousal value below -250.\n",
            "Skipping C5_S129_P258_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S130_P259_AD4/Round1_SessionId_123_VideoId_8_UserId_91470114_cab1_40f6_8dec_c648eb3aa363_Arousal_Arousal1.csv\n",
            "Skipping C5_S130_P260_AD4: No arousal value below -250.\n",
            "Skipping C5_S131_P261_AD4: No arousal value below -250.\n",
            "Skipping C5_S131_P262_AD4: No arousal value below -250.\n",
            "Skipping C5_S132_P263_AD4: No arousal value below -250.\n",
            "Skipping C5_S132_P264_AD4: No arousal value below -250.\n",
            "Skipping C5_S133_P265_AD4: No arousal value below -250.\n",
            "Skipping C5_S133_P266_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S134_P267_AD4/Round1_SessionId_217_VideoId_8_UserId_400086af_d888_4367_a19c_315045a85e81_Arousal_Arousal1.csv\n",
            "Skipping C5_S134_P268_AD4: No arousal value below -250.\n",
            "Skipping C5_S135_P269_AD4: No arousal value below -250.\n",
            "Skipping C5_S135_P270_AD4: No arousal value below -250.\n",
            "Skipping C5_S136_P271_AD4: No arousal value below -250.\n",
            "Skipping C5_S136_P272_AD4: No arousal value below -250.\n",
            "Skipping C5_S137_P273_AD4: No arousal value below -250.\n",
            "Skipping C5_S137_P274_AD4: No arousal value below -250.\n",
            "Skipping C5_S138_P275_AD4: No arousal value below -250.\n",
            "Skipping C5_S138_P276_AD4: No arousal value below -250.\n",
            "Skipping C5_S139_P277_AD4: No arousal value below -250.\n",
            "Skipping C5_S139_P278_AD4: No arousal value below -250.\n",
            "Skipping C5_S140_P279_AD4: No arousal value below -250.\n",
            "Skipping C5_S140_P280_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S141_P281_AD4/Round1_SessionId_254_VideoId_8_UserId_9d836dbc_1610_4f0b_bf00_4ce820a32ef3_Arousal_Arousal1.csv\n",
            "Skipping C5_S141_P282_AD4: No arousal value below -250.\n",
            "Skipping C5_S142_P283_AD4: No arousal value below -250.\n",
            "Skipping C5_S142_P284_AD4: No arousal value below -250.\n",
            "Skipping C5_S143_P285_AD4: No arousal value below -250.\n",
            "Skipping C5_S143_P286_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S144_P287_AD4/Round1_SessionId_304_VideoId_8_UserId_974ba6f2_5683_421c_b28b_a25f5c720b9c_Arousal_Arousal1.csv\n",
            "Skipping C5_S144_P288_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S145_P289_AD4/Round1_SessionId_342_VideoId_8_UserId_2d9d8ea0_f3c4_4fd1_88f7_87ac5ff2968e_Arousal_Arousal1.csv\n",
            "Skipping C5_S145_P290_AD4: No arousal value below -250.\n",
            "Skipping C5_S146_P291_AD4: No arousal value below -250.\n",
            "Skipping C5_S146_P292_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S147_P293_AD4/Round1_SessionId_376_VideoId_8_UserId_e42da593_f3ef_4f48_86e4_4b3917bb97f2_Arousal_Arousal1.csv\n",
            "Skipping C5_S147_P294_AD4: No arousal value below -250.\n",
            "Skipping C5_S148_P295_AD4: No arousal value below -250.\n",
            "Skipping C5_S148_P296_AD4: No arousal value below -250.\n",
            "Skipping C5_S149_P297_AD4: No arousal value below -250.\n",
            "Skipping C5_S149_P298_AD4: No arousal value below -250.\n",
            "Skipping C5_S150_P299_AD4: No arousal value below -250.\n",
            "Skipping C5_S150_P300_AD4: No arousal value below -250.\n",
            "Skipping C5_S151_P301_AD4: No arousal value below -250.\n",
            "Skipping C5_S151_P302_AD4: No arousal value below -250.\n",
            "Skipping C5_S152_P303_AD4: No arousal value below -250.\n",
            "Skipping C5_S152_P304_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S153_P305_AD4/Round1_SessionId_443_VideoId_8_UserId_33aae94a_56d8_44f2_995b_7e567f6e44c2_Arousal_Arousal1.csv\n",
            "Skipping C5_S153_P306_AD4: No arousal value below -250.\n",
            "Skipping C5_S154_P307_AD4: No arousal value below -250.\n",
            "Skipping C5_S154_P308_AD4: No arousal value below -250.\n",
            "Skipping C5_S155_P309_AD4: No arousal value below -250.\n",
            "Skipping C5_S155_P310_AD4: No arousal value below -250.\n",
            "Skipping C5_S156_P311_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S156_P312_AD4/Round1_SessionId_462_VideoId_8_UserId_96acfd4c_7c5f_4475_82a6_3db638391b82_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S157_P313_AD4/Round1_SessionId_474_VideoId_8_UserId_ade8c0ec_6ed3_496d_bdea_cf62bb253db5_Arousal_Arousal1.csv\n",
            "Skipping C5_S157_P314_AD4: No arousal value below -250.\n",
            "Skipping C5_S158_P315_AD4: No arousal value below -250.\n",
            "Skipping C5_S158_P316_AD4: No arousal value below -250.\n",
            "Skipping C5_S159_P317_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S159_P318_AD4/Round1_SessionId_405_VideoId_8_UserId_91263ece_1f63_4de0_b68a_e1b3fae86b78_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S160_P319_AD4/Round1_SessionId_489_VideoId_8_UserId_9fbab926_5b79_4200_9e48_d7ec60fce49f_Arousal_Arousal1.csv\n",
            "Skipping C5_S160_P320_AD4: No arousal value below -250.\n",
            "Skipping C5_S161_P321_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S161_P322_AD4/Round1_SessionId_495_VideoId_8_UserId_97aad3a9_ab7d_49a3_84f1_6cdfd198a2e5_Arousal_Arousal1.csv\n",
            "Skipping C5_S162_P323_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C5_S162_P324_AD4/Round1_SessionId_501_VideoId_8_UserId_032a6415_a62d_456e_834e_12bd735fbae2_Arousal_Arousal1.csv\n",
            "Skipping C5_S163_P325_AD4: No arousal value below -250.\n",
            "Skipping C5_S163_P326_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S164_P327_AD4/Round1_SessionId_189_VideoId_4_UserId_85ccfa46_31c0_4648_ad0a_c59eeabbdc2e_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S164_P328_AD4/Round1_SessionId_189_VideoId_4_UserId_82b83097_1543_482b_88bc_2c3e83d51dc5_Arousal_Arousal1.csv\n",
            "Skipping C6_S165_P329_AD4: No arousal value below -250.\n",
            "Skipping C6_S165_P330_AD4: No arousal value below -250.\n",
            "Skipping C6_S166_P331_AD4: No arousal value below -250.\n",
            "Skipping C6_S166_P332_AD4: No arousal value below -250.\n",
            "Skipping C6_S167_P333_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S167_P334_AD4/Round1_SessionId_243_VideoId_4_UserId_b19112d2_ad5b_49ba_bb82_5e38a623d900_Arousal_Arousal1.csv\n",
            "Skipping C6_S168_P335_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S168_P336_AD4/Round1_SessionId_245_VideoId_4_UserId_8112d063_287b_4f78_8a93_c5888b875199_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S169_P337_AD4/Round1_SessionId_246_VideoId_4_UserId_7cca8833_b08a_43d6_95fd_5a242c538988_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S169_P338_AD4/Round1_SessionId_246_VideoId_4_UserId_e4bb5410_2fb8_4dec_91d7_057e89bebfc9_Arousal_Arousal1.csv\n",
            "Skipping C6_S170_P339_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S170_P340_AD4/Round1_SessionId_249_VideoId_4_UserId_2cdbce0b_13f4_48f0_9dce_3c3b03ef6b30_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S171_P341_AD4/Round1_SessionId_250_VideoId_4_UserId_71444fe1_c651_4bf9_85d8_903e7ffd5e45_Arousal_Arousal1.csv\n",
            "Skipping C6_S171_P342_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S172_P343_AD4/Round1_SessionId_252_VideoId_4_UserId_6b153ef7_90c9_4a8d_b857_3d8c1dfc2207_Arousal_Arousal1.csv\n",
            "Skipping C6_S172_P344_AD4: No arousal value below -250.\n",
            "Skipping C6_S173_P345_AD4: No arousal value below -250.\n",
            "Skipping C6_S173_P346_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S174_P347_AD4/Round1_SessionId_256_VideoId_4_UserId_9b4cbbba_339c_4bdc_93bb_f1e5a29d2d4f_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S174_P348_AD4/Round1_SessionId_256_VideoId_4_UserId_7575f8fd_77d1_4f99_bee2_7e8fc55b9edf_Arousal_Arousal1.csv\n",
            "Skipping C6_S175_P349_AD4: No arousal value below -250.\n",
            "Skipping C6_S175_P350_AD4: No arousal value below -250.\n",
            "Skipping C6_S176_P351_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S176_P352_AD4/Round1_SessionId_262_VideoId_4_UserId_474e9624_3138_417f_8004_300670aa24a6_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S177_P353_AD4/Round1_SessionId_264_VideoId_4_UserId_93da8c2d_2c5f_4d4f_890d_33012d9e21ff_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S177_P354_AD4/Round1_SessionId_264_VideoId_4_UserId_72277925_ad47_452d_9414_82be3a20b26b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S178_P355_AD4/Round1_SessionId_265_VideoId_4_UserId_c7440572_7a25_4252_9edc_6942fd85803b_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S178_P356_AD4/Round1_SessionId_265_VideoId_4_UserId_ba686aa9_6451_4be6_adf9_991c413564cc_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S179_P357_AD4/Round1_SessionId_274_VideoId_4_UserId_81e10e35_a80a_4710_a3a3_4390dbfd9c75_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S179_P358_AD4/Round1_SessionId_274_VideoId_4_UserId_66417830_1a58_4835_8f3b_6c6ca5af2cc4_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S180_P359_AD4/Round1_SessionId_277_VideoId_4_UserId_dfd2bdb6_85ff_41db_bd77_3f9669f62e95_Arousal_Arousal1.csv\n",
            "Skipping C6_S180_P360_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S181_P361_AD4/Round1_SessionId_290_VideoId_4_UserId_c442440e_c6e1_4568_86c3_8b831b19bdc0_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S181_P362_AD4/Round1_SessionId_290_VideoId_4_UserId_ea24fed7_8886_4e86_9039_d38076f20119_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S182_P363_AD4/Round1_SessionId_380_VideoId_4_UserId_5c2c488d_a502_4786_aacc_f00bf81cee84_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S182_P364_AD4/Round1_SessionId_380_VideoId_4_UserId_b620fd75_e2ec_4cdc_b6d0_1ed8c675ed6d_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S183_P365_AD4/Round1_SessionId_158_VideoId_4_UserId_9f164ef4_a7e1_40ee_9d7b_76e6c7af4617_Arousal_Arousal1.csv\n",
            "Skipping C6_S183_P366_AD4: No arousal value below -250.\n",
            "Skipping C6_S184_P367_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S184_P368_AD4/Round1_SessionId_232_VideoId_4_UserId_d22e3d0a_8652_49ad_a6a8_dacff5f9c90c_Arousal_Arousal1.csv\n",
            "Skipping C6_S185_P369_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S185_P370_AD4/Round1_SessionId_432_VideoId_4_UserId_6ee4e836_33a8_4b5e_bbce_d7e58356cdb7_Arousal_Arousal1.csv\n",
            "Skipping C6_S186_P371_AD4: No arousal value below -250.\n",
            "Skipping C6_S186_P372_AD4: No arousal value below -250.\n",
            "Skipping C6_S187_P373_AD4: No arousal value below -250.\n",
            "Skipping C6_S187_P374_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S188_P375_AD4/Round1_SessionId_494_VideoId_4_UserId_228f85d6_9de8_4885_a964_e41c1fabbcda_Arousal_Arousal1.csv\n",
            "Skipping C6_S188_P376_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S189_P377_AD4/Round1_SessionId_498_VideoId_4_UserId_b515f78a_e0f6_4873_ae5e_508675fad624_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S189_P378_AD4/Round1_SessionId_498_VideoId_4_UserId_10eb7f9d_d6d9_4f63_adde_8bab5ee47956_Arousal_Arousal1.csv\n",
            "Skipping C6_S190_P379_AD4: No arousal value below -250.\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S190_P380_AD4/Round1_SessionId_499_VideoId_4_UserId_d13af71f_2626_4595_8439_7d8e978e2752_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S191_P381_AD4/Round1_SessionId_500_VideoId_4_UserId_68feca80_7032_411c_accc_8ae60b87cb28_Arousal_Arousal1.csv\n",
            "Processed and saved: /content/drive/MyDrive/SEWA(AOML)1/C6_S191_P382_AD4/Round1_SessionId_500_VideoId_4_UserId_e36abdf9_e3e7_4458_9d09_b1a5c65c3c85_Arousal_Arousal1.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Helper function to find the arousal file by prefix\n",
        "def find_arousal_file(subject_folder):\n",
        "    for file in os.listdir(subject_folder):\n",
        "        if \"Arousal\" in file:  # Match prefix 'Arousal'\n",
        "            return os.path.join(subject_folder, file)\n",
        "    raise FileNotFoundError(f\"No file with prefix 'Arousal' found in {subject_folder}\")\n",
        "\n",
        "# Function to process arousal CSV files and save below-threshold values\n",
        "def process_arousal_csvs(input_dir, output_dir, threshold=-250):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    subjects = sorted(os.listdir(input_dir))\n",
        "    for subject in subjects:\n",
        "        subject_folder = os.path.join(input_dir, subject)\n",
        "        if not os.path.isdir(subject_folder):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            arousal_file = find_arousal_file(subject_folder)\n",
        "            arousal_data = pd.read_csv(arousal_file)\n",
        "\n",
        "            # Filter rows with arousal value below the threshold\n",
        "            filtered_data = arousal_data[arousal_data['arousal'] < threshold].copy()\n",
        "            if filtered_data.empty:\n",
        "                print(f\"Skipping {subject}: No arousal value below {threshold}.\")\n",
        "                continue\n",
        "\n",
        "            filtered_data['arousal'] = 1  # Set arousal to 1 as per requirement\n",
        "\n",
        "            # Save the processed data to a new CSV file\n",
        "            subject_output_folder = os.path.join(output_dir, subject)\n",
        "            if not os.path.exists(subject_output_folder):\n",
        "                os.makedirs(subject_output_folder)\n",
        "\n",
        "            output_csv_path = os.path.join(subject_output_folder, f\"{os.path.basename(arousal_file).replace('.csv', '_Arousal1.csv')}\")\n",
        "            filtered_data.to_csv(output_csv_path, index=False)\n",
        "\n",
        "            print(f\"Processed and saved: {output_csv_path}\")\n",
        "        except FileNotFoundError as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "# Dataset class for balanced extraction\n",
        "class BalancedAttentionDataset(Dataset):\n",
        "    def __init__(self, data_dir, fixed_sequence_length=12, transform=None, frame_rate=10):\n",
        "        self.data_dir = data_dir\n",
        "        self.fixed_sequence_length = fixed_sequence_length\n",
        "        self.transform = transform\n",
        "        self.frame_rate = frame_rate\n",
        "        self.subjects = sorted(os.listdir(data_dir))[:390]  # Use the first 390 subjects\n",
        "        self.data = self.preprocess_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        processed_data = []\n",
        "        for subject in self.subjects:\n",
        "            subject_folder = os.path.join(self.data_dir, subject)\n",
        "            video_file = next((f for f in os.listdir(subject_folder) if f.endswith('.avi')), None)\n",
        "            if not video_file:\n",
        "                print(f\"No video file found in {subject_folder}\")\n",
        "                continue\n",
        "            video_path = os.path.join(subject_folder, video_file)\n",
        "            try:\n",
        "                arousal_path = find_arousal_file(subject_folder)\n",
        "                arousal_data = pd.read_csv(arousal_path)\n",
        "                attentive_times = [row['time'] for _, row in arousal_data.iterrows() if row['arousal'] >= -250]\n",
        "                inattentive_times = [row['time'] for _, row in arousal_data.iterrows() if row['arousal'] < -250]\n",
        "                if not attentive_times or not inattentive_times:\n",
        "                    print(f\"Skipping {subject}: Only one class present.\")\n",
        "                    continue\n",
        "                minority, majority = (inattentive_times, attentive_times) if len(inattentive_times) < len(attentive_times) else (attentive_times, inattentive_times)\n",
        "                grouped_majority = self.group_consecutive(majority, len(minority))\n",
        "                if len(grouped_majority) < len(minority):\n",
        "                    grouped_majority = self.group_consecutive(majority, 1)\n",
        "                sampled_majority = grouped_majority[:len(minority)]\n",
        "                processed_data.append({\n",
        "                    \"video_path\": video_path,\n",
        "                    \"arousal_path\": arousal_path,\n",
        "                    \"attentive_times\": sampled_majority,\n",
        "                    \"inattentive_times\": minority\n",
        "                })\n",
        "            except FileNotFoundError as e:\n",
        "                print(e)\n",
        "                continue\n",
        "        return processed_data\n",
        "\n",
        "    def group_consecutive(self, times, group_size):\n",
        "        grouped = []\n",
        "        temp_group = [times[0]]\n",
        "        for i in range(1, len(times)):\n",
        "            if times[i] - times[i - 1] == 1:\n",
        "                temp_group.append(times[i])\n",
        "            else:\n",
        "                grouped.append(temp_group)\n",
        "                temp_group = [times[i]]\n",
        "        grouped.append(temp_group)\n",
        "        flat_groups = [group[:group_size] for group in grouped if len(group) >= group_size]\n",
        "        return flat_groups\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        video_path = entry['video_path']\n",
        "        arousal_data = pd.read_csv(entry['arousal_path'])\n",
        "        if np.random.rand() > 0.5:\n",
        "            times = entry['inattentive_times']\n",
        "            label = 0\n",
        "        else:\n",
        "            times = entry['attentive_times']\n",
        "            label = 1\n",
        "        frames = self.extract_frames(video_path, arousal_data, times)\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "        return torch.stack(frames[:self.fixed_sequence_length], 0), torch.tensor(label)\n",
        "\n",
        "    def extract_frames(self, video_path, arousal_data, times):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "        frames = []\n",
        "        for time_group in times:\n",
        "            if not isinstance(time_group, list):\n",
        "                time_group = [time_group]\n",
        "            for time in time_group:\n",
        "                frame_idx = int(time * fps)\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    continue\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
        "                frames.append(frame)\n",
        "        cap.release()\n",
        "        while len(frames) < self.fixed_sequence_length:\n",
        "            frames.append(torch.zeros((3, 224, 224)))\n",
        "        return frames[:self.fixed_sequence_length]\n",
        "\n",
        "# Define ViT-based Model for Attention Detection\n",
        "class ViTAttentionDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTAttentionDetector, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.vit(x)\n",
        "        x = x.view(B, T, -1)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x.view(-1)\n",
        "\n",
        "# Instantiate and process dataset\n",
        "input_dir = \"/content/drive/MyDrive/SEWA(AOML)\"\n",
        "output_dir = \"/content/drive/MyDrive/SEWA(AOML)1\"\n",
        "process_arousal_csvs(input_dir, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_arousal2_files(input_folder, output_folder, total_rows_required=9712, max_rows_per_file=25):\n",
        "    # Ensure the output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    # Step 1: Collect eligible rows from all files\n",
        "    for subject_folder in os.listdir(input_folder):\n",
        "        subject_path = os.path.join(input_folder, subject_folder)\n",
        "\n",
        "        # Ensure it's a directory\n",
        "        if os.path.isdir(subject_path):\n",
        "            for file_name in os.listdir(subject_path):\n",
        "                if file_name.endswith(\"Arousal.csv\"):  # Process only Arousal CSV files\n",
        "                    file_path = os.path.join(subject_path, file_name)\n",
        "\n",
        "                    try:\n",
        "                        # Read the CSV file\n",
        "                        df = pd.read_csv(file_path)\n",
        "\n",
        "                        # Ensure the required column exists\n",
        "                        if 'arousal' not in df.columns:\n",
        "                            print(f\"Skipping {file_path}: Missing 'arousal' column.\")\n",
        "                            continue\n",
        "\n",
        "                        # Filter rows with arousal > -250\n",
        "                        eligible_rows = df[df['arousal'] > -250]\n",
        "\n",
        "                        # Limit to max rows per file\n",
        "                        all_rows.extend(eligible_rows.head(max_rows_per_file).values.tolist())\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    # Step 2: Cycle back if needed to reach total rows required\n",
        "    row_count = len(all_rows)\n",
        "    while row_count < total_rows_required:\n",
        "        for subject_folder in os.listdir(input_folder):\n",
        "            subject_path = os.path.join(input_folder, subject_folder)\n",
        "            if os.path.isdir(subject_path):\n",
        "                for file_name in os.listdir(subject_path):\n",
        "                    if file_name.endswith(\"Arousal.csv\"):  # Process only Arousal CSV files\n",
        "                        file_path = os.path.join(subject_path, file_name)\n",
        "\n",
        "                        try:\n",
        "                            # Read the CSV file\n",
        "                            df = pd.read_csv(file_path)\n",
        "                            if 'arousal' not in df.columns:\n",
        "                                continue\n",
        "\n",
        "                            # Filter rows with arousal > -250 and skip already used rows\n",
        "                            unused_rows = df[df['arousal'] > -250]\n",
        "                            unused_rows = unused_rows.iloc[max_rows_per_file:]\n",
        "                            all_rows.extend(unused_rows.values.tolist())\n",
        "                            row_count = len(all_rows)\n",
        "\n",
        "                            if row_count >= total_rows_required:\n",
        "                                break\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "            if row_count >= total_rows_required:\n",
        "                break\n",
        "\n",
        "    # Step 3: Create new CSV files in the output folder\n",
        "    processed_rows = 0\n",
        "    for subject_folder in os.listdir(input_folder):\n",
        "        subject_path = os.path.join(input_folder, subject_folder)\n",
        "        if os.path.isdir(subject_path):\n",
        "            output_subject_path = os.path.join(output_folder, subject_folder)\n",
        "            os.makedirs(output_subject_path, exist_ok=True)\n",
        "\n",
        "            for file_name in os.listdir(subject_path):\n",
        "                if file_name.endswith(\"Arousal.csv\"):  # Process only Arousal CSV files\n",
        "                    file_path = os.path.join(subject_path, file_name)\n",
        "                    output_file_name = f\"{os.path.splitext(file_name)[0]}_Arousal2.csv\"\n",
        "                    output_file_path = os.path.join(output_subject_path, output_file_name)\n",
        "\n",
        "                    # Select rows for this file\n",
        "                    rows_to_write = all_rows[processed_rows:processed_rows + max_rows_per_file]\n",
        "                    processed_rows += len(rows_to_write)\n",
        "\n",
        "                    # If no rows are left to write, skip\n",
        "                    if not rows_to_write:\n",
        "                        continue\n",
        "\n",
        "                    # Write rows to the new CSV file\n",
        "                    df_out = pd.DataFrame(rows_to_write, columns=[\"time\", \"arousal\"])\n",
        "                    df_out[\"arousal\"] = 0  # Set arousal value to 0\n",
        "                    df_out.to_csv(output_file_path, index=False)\n",
        "\n",
        "                    # Stop if all required rows are processed\n",
        "                    if processed_rows >= total_rows_required:\n",
        "                        break\n",
        "            if processed_rows >= total_rows_required:\n",
        "                break\n",
        "\n",
        "    print(f\"Processed rows: {processed_rows}\")\n",
        "    print(f\"New CSV files created in: {output_folder}\")\n",
        "\n",
        "\n",
        "# Input and output folder paths\n",
        "input_folder_path = \"/content/drive/MyDrive/SEWA(AOML)\"\n",
        "output_folder_path = \"/content/drive/MyDrive/SEWA(AOML)2\"\n",
        "\n",
        "# Run the function\n",
        "create_arousal2_files(input_folder_path, output_folder_path)\n"
      ],
      "metadata": {
        "id": "6Hkz7Y1Fqnt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5cbab3-deae-466e-8311-6a95cec78d0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed rows: 9725\n",
            "New CSV files created in: /content/drive/MyDrive/SEWA(AOML)2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import torch.cuda.amp as amp\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BalancedAttentionDataset(Dataset):\n",
        "    def __init__(self, video_dir, attentive_dir, notattentive_dir, fixed_sequence_length=12, transform=None, frame_rate=50, max_samples=1000):\n",
        "        self.video_dir = video_dir\n",
        "        self.attentive_dir = attentive_dir\n",
        "        self.notattentive_dir = notattentive_dir\n",
        "        self.fixed_sequence_length = fixed_sequence_length\n",
        "        self.transform = transform\n",
        "        self.frame_rate = frame_rate\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        self.subjects = sorted(os.listdir(video_dir))\n",
        "        self.data = self.preprocess_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        processed_data = []\n",
        "        attentive_count = 0\n",
        "        non_attentive_count = 0\n",
        "\n",
        "        # Use tqdm for preprocessing progress\n",
        "        for subject in tqdm(self.subjects, desc=\"Processing Subjects\"):\n",
        "            # Early stopping if we've reached max samples\n",
        "            if attentive_count + non_attentive_count >= self.max_samples:\n",
        "                break\n",
        "\n",
        "            video_path = os.path.join(self.video_dir, subject)\n",
        "            if not os.path.isdir(video_path):\n",
        "                continue\n",
        "\n",
        "            # Match video file\n",
        "            video_file = next((f for f in os.listdir(video_path) if f.endswith('.avi')), None)\n",
        "            if not video_file:\n",
        "                continue\n",
        "\n",
        "            video_path = os.path.join(video_path, video_file)\n",
        "\n",
        "            # Match attentive CSV\n",
        "            attentive_path = os.path.join(self.attentive_dir, subject)\n",
        "            attentive_file = next((f for f in os.listdir(attentive_path) if f.endswith('_Arousal2.csv')), None) if os.path.exists(attentive_path) else None\n",
        "\n",
        "            # Match non-attentive CSV\n",
        "            notattentive_path = os.path.join(self.notattentive_dir, subject)\n",
        "            notattentive_file = next((f for f in os.listdir(notattentive_path) if f.endswith('_Arousal1.csv')), None) if os.path.exists(notattentive_path) else None\n",
        "\n",
        "            # Process attentive times\n",
        "            if attentive_file and attentive_count < self.max_samples // 2:\n",
        "                attentive_csv = pd.read_csv(os.path.join(attentive_path, attentive_file))\n",
        "                for time in attentive_csv['time']:\n",
        "                    if attentive_count < self.max_samples // 2:\n",
        "                        processed_data.append({\n",
        "                            'video_path': video_path,\n",
        "                            'start_time': time,\n",
        "                            'label': 0  # Attentive\n",
        "                        })\n",
        "                        attentive_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            # Process non-attentive times\n",
        "            if notattentive_file and non_attentive_count < self.max_samples // 2:\n",
        "                notattentive_csv = pd.read_csv(os.path.join(notattentive_path, notattentive_file))\n",
        "                for time in notattentive_csv['time']:\n",
        "                    if non_attentive_count < self.max_samples // 2:\n",
        "                        processed_data.append({\n",
        "                            'video_path': video_path,\n",
        "                            'start_time': time,\n",
        "                            'label': 1  # Not Attentive\n",
        "                        })\n",
        "                        non_attentive_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "        print(f\"Processed Attentive Samples: {attentive_count}\")\n",
        "        print(f\"Processed Non-Attentive Samples: {non_attentive_count}\")\n",
        "        return processed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        video_path = entry['video_path']\n",
        "        start_time = entry['start_time']\n",
        "        label = entry['label']\n",
        "\n",
        "        frames = self.extract_frames(video_path, start_time)\n",
        "\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        return torch.stack(frames[:self.fixed_sequence_length], 0), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "    def extract_frames(self, video_path, start_time):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or self.frame_rate\n",
        "        frames = []\n",
        "\n",
        "        # Extract frames for the specific second\n",
        "        frame_idx = int(start_time * fps)\n",
        "        for offset in range(self.fixed_sequence_length):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx + offset)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            else:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "\n",
        "            frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "class ViTAttentionDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTAttentionDetector, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.vit(x)\n",
        "        x = x.view(B, T, -1)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x.view(-1)\n",
        "\n",
        "# Split dataset into train and validation\n",
        "def split_dataset(dataset, train_ratio=0.8):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Main Training Script\n",
        "def main():\n",
        "    # Directories\n",
        "    video_dir = \"/content/drive/MyDrive/SEWA(AOML)\"\n",
        "    attentive_dir = \"/content/drive/MyDrive/SEWA(AOML)2\"\n",
        "    notattentive_dir = \"/content/drive/MyDrive/SEWA(AOML)1\"\n",
        "\n",
        "    # Transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create Dataset\n",
        "    dataset = BalancedAttentionDataset(\n",
        "        video_dir,\n",
        "        attentive_dir,\n",
        "        notattentive_dir,\n",
        "        transform=transform,\n",
        "        max_samples=1000  # Limit samples to reduce processing time\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = split_dataset(dataset)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Model\n",
        "    model = ViTAttentionDetector().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    # Training\n",
        "    scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(10):  # Reduced epochs\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
        "\n",
        "        for frames, labels in progress_bar:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast() if device == 'cuda' else torch.no_grad():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Train Loss': f'{loss.item():.4f}',\n",
        "            })\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for frames, labels in val_progress_bar:\n",
        "                frames, labels = frames.to(device), labels.to(device)\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.sigmoid(outputs) >= 0.5\n",
        "                correct += (preds == labels).float().sum()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Update validation progress bar\n",
        "                val_progress_bar.set_postfix({\n",
        "                    'Val Loss': f'{loss.item():.4f}',\n",
        "                })\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, \"\n",
        "              f\"Val Loss = {val_loss/len(val_loader):.4f}, \"\n",
        "              f\"Val Accuracy = {correct/total:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss/len(val_loader) < best_val_loss:\n",
        "            best_val_loss = val_loss/len(val_loader)\n",
        "            torch.save(model.state_dict(), 'best_attention_model.pth')\n",
        "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "f6e8f81d9c4f41acb7480b46befe39a6",
            "523b47490b1b4196a4f9fae3e0a2a661",
            "6f184aa18cc34d2fb626ccaa4c7ff38d",
            "2fb1cd90ca5e4e70a124761b0a9195cc",
            "7338958a8265424791157f036e88715d",
            "0b251f9afa7b4bbc9dc0542d5f15f1e9",
            "5cf9d5fad2eb41a38837869e1208c32a",
            "c2643a4c0509463ea2315b77102a98e9",
            "3ee4953033c0474482ffd2be63ee292e",
            "0d8d729f8ec141a29c73a6d95afa81c6",
            "29dafd541b7d4fda9ddd50bb58cb12db"
          ]
        },
        "id": "RbeXRaz345DU",
        "outputId": "237b431a-7b95-4907-a07b-d365178e1a51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Subjects:  22%|██▏       | 88/397 [00:00<00:00, 344.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed Attentive Samples: 500\n",
            "Processed Non-Attentive Samples: 500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6e8f81d9c4f41acb7480b46befe39a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5f0b8de404a8>:189: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
            "Epoch 1:   0%|          | 0/50 [00:00<?, ?batch/s]<ipython-input-4-5f0b8de404a8>:203: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast() if device == 'cuda' else torch.no_grad():\n",
            "Epoch 1: 100%|██████████| 50/50 [08:45<00:00, 10.52s/batch, Train Loss=0.0119]\n",
            "Validation: 100%|██████████| 13/13 [02:18<00:00, 10.66s/batch, Val Loss=0.0093]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.1639, Val Loss = 0.0387, Val Accuracy = 0.9900\n",
            "Saved new best model with validation loss: 0.0387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 50/50 [08:35<00:00, 10.31s/batch, Train Loss=0.0001]\n",
            "Validation: 100%|██████████| 13/13 [02:15<00:00, 10.43s/batch, Val Loss=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 0.0351, Val Loss = 0.0355, Val Accuracy = 0.9950\n",
            "Saved new best model with validation loss: 0.0355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ViTAttentionDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTAttentionDetector, self).__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.vit(x)\n",
        "        x = x.view(B, T, -1)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x.view(-1)\n",
        "\n",
        "def test_video(video_path, model_path='best_attention_model.pth'):\n",
        "    \"\"\"\n",
        "    Test video for attention detection\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): Path to the video file to test\n",
        "    - model_path (str): Path to the saved model weights\n",
        "\n",
        "    Returns:\n",
        "    - List of attention predictions\n",
        "    \"\"\"\n",
        "    # Device setup\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Load model\n",
        "    model = ViTAttentionDetector().to(device)\n",
        "\n",
        "    # Use weights_only=True for security and to match how the model was likely saved\n",
        "    try:\n",
        "        # First, try loading state_dict directly\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    except Exception:\n",
        "        # If that fails, try loading with weights_only=True\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Video capture\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 50\n",
        "    total_seconds = total_frames / fps\n",
        "\n",
        "    # Transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Prediction storage\n",
        "    attention_results = []\n",
        "\n",
        "    # Process video second by second\n",
        "    for second in range(int(total_seconds)):\n",
        "        # Extract frames for this second\n",
        "        frames = []\n",
        "        for offset in range(12):  # 12 frames per second\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, int((second + offset/12) * fps))\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            else:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "\n",
        "            frame = transform(frame)\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Convert frames to tensor\n",
        "        frames_tensor = torch.stack(frames).unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict attention\n",
        "        with torch.no_grad():\n",
        "            output = model(frames_tensor)\n",
        "            probability = torch.sigmoid(output).cpu().numpy()[0]\n",
        "\n",
        "            # Classify as attentive or not attentive\n",
        "            is_attentive = probability > 0.5\n",
        "\n",
        "            attention_results.append({\n",
        "                'second': second,\n",
        "                'is_attentive': is_attentive,\n",
        "                'probability': probability\n",
        "            })\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return attention_results\n",
        "\n",
        "def print_attention_summary(attention_results):\n",
        "    \"\"\"\n",
        "    Print summary of attention detection results\n",
        "\n",
        "    Args:\n",
        "    - attention_results (list): List of attention prediction dictionaries\n",
        "    \"\"\"\n",
        "    print(\"\\nAttention Detection Summary:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    attentive_seconds = [\n",
        "        result['second'] for result in attention_results if result['is_attentive']\n",
        "    ]\n",
        "    non_attentive_seconds = [\n",
        "        result['second'] for result in attention_results if not result['is_attentive']\n",
        "    ]\n",
        "\n",
        "    print(\"Attentive Seconds:\")\n",
        "    print(\", \".join(map(str, attentive_seconds)) if attentive_seconds else \"None\")\n",
        "\n",
        "    print(\"\\nNon-Attentive Seconds:\")\n",
        "    print(\", \".join(map(str, non_attentive_seconds)) if non_attentive_seconds else \"None\")\n",
        "\n",
        "    print(f\"\\nTotal Seconds Analyzed: {len(attention_results)}\")\n",
        "    print(f\"Attentive Seconds: {len(attentive_seconds)}\")\n",
        "    print(f\"Non-Attentive Seconds: {len(non_attentive_seconds)}\")\n",
        "    print(f\"Attentiveness Percentage: {len(attentive_seconds)/len(attention_results)*100:.2f}%\")\n",
        "\n",
        "# Typical usage\n",
        "def analyze_video(video_path):\n",
        "    \"\"\"\n",
        "    Main function to analyze video attention\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): Path to the video file\n",
        "\n",
        "    Returns:\n",
        "    - List of attention results\n",
        "    \"\"\"\n",
        "    # Test the video\n",
        "    attention_results = test_video(video_path)\n",
        "\n",
        "    # Print summary\n",
        "    print_attention_summary(attention_results)\n",
        "\n",
        "    return attention_results\n",
        "\n",
        "# If you want to use this as a standalone script, uncomment the following:\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = \"/content/drive/MyDrive/SEWA(AOML)/C4_S108_P215_AD4/Round1_SessionId_206_VideoId_4_UserId_0d2867ac-de24-49f9-85ce-15cb4c5c1d9c.avi\"\n",
        "    analyze_video(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kumY5L_LBQWn",
        "outputId": "09503dd1-e9d6-489a-ea69-0fc3ef719c6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-cfa629414cc8>:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attention Detection Summary:\n",
            "----------------------------------------\n",
            "Attentive Seconds:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51\n",
            "\n",
            "Non-Attentive Seconds:\n",
            "50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93\n",
            "\n",
            "Total Seconds Analyzed: 94\n",
            "Attentive Seconds: 51\n",
            "Non-Attentive Seconds: 43\n",
            "Attentiveness Percentage: 54.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tLTqPQ2OF9Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move('best_attention_model.pth', '/content/drive/MyDrive/v9_e2.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oHadrNDxETWb",
        "outputId": "d762573e-caa6-40ab-fc20-271f1bfd0351"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/v9_e2.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}